{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71099615\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def process_headline(headline):\n",
    "    processed = headline.replace('Â\\xa0', ' ').replace('â\\x80\\x98', \"'\")\n",
    "    if ';;;' in processed:\n",
    "        processed = processed.split(';;;')[0]\n",
    "    return processed\n",
    "\n",
    "# Assuming the CSV file is named 'data.csv' and has the correct format\n",
    "file_name = 'filteredDataBGlobe.tsv'\n",
    "\n",
    "userIDsTrain = []\n",
    "articleHeadlinesTrain = []\n",
    "datesTrain = []\n",
    "\n",
    "userIDsValidation = []\n",
    "articleHeadlinesValidation = []\n",
    "datesValidation = []\n",
    "\n",
    "userIDsTest = []\n",
    "articleHeadlinesTest = []\n",
    "datesTest = []\n",
    "\n",
    "with open(file_name, mode='r', encoding='utf-8') as file:\n",
    "    size = 0\n",
    "    tsv_reader = csv.reader(file, delimiter='\\t')\n",
    "    next(tsv_reader)  \n",
    "    for row in tsv_reader:\n",
    "        size += 1\n",
    "        userID, headline, date = row[0], row[1], row[2]\n",
    "        if headline.strip(): \n",
    "            if (date[0:4] == \"2017\"):\n",
    "                userIDsTrain.append(userID)\n",
    "                processed = process_headline(headline)\n",
    "                articleHeadlinesTrain.append(processed)\n",
    "                datesTrain.append(date)\n",
    "            elif (date[0:4] == \"2018\" and int(date[5:7]) < 7 ) : \n",
    "                userIDsValidation.append(userID)\n",
    "                processed = process_headline(headline)\n",
    "                articleHeadlinesValidation.append(processed)\n",
    "                datesValidation.append(date)\n",
    "            else : \n",
    "                userIDsTest.append(userID)\n",
    "                processed = process_headline(headline)\n",
    "                articleHeadlinesTest.append(processed)\n",
    "                datesTest.append(date)\n",
    "    print (size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16931101\n"
     ]
    }
   ],
   "source": [
    "print(len(userIDsTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (user_ids[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_headlines[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(consumption_times[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'userIDs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      3\u001b[0m unique_user_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43muserIDs\u001b[49m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m unique_user_ids:\n\u001b[1;32m      6\u001b[0m         unique_user_ids\u001b[38;5;241m.\u001b[39madd(user_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'userIDs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def filter_data(userIDs, articleHeadlines, dates, limit):\n",
    "    user_dates = {}\n",
    "    for user_id, date in zip(userIDs, dates):\n",
    "        if user_id not in user_dates:\n",
    "            user_dates[user_id] = set()\n",
    "        if date not in user_dates[user_id]:\n",
    "            user_dates[user_id].add(date)\n",
    "\n",
    "    multiple_dates_users = {user for user, dates in user_dates.items() if len(dates) > 1}\n",
    "\n",
    "    filtered_data = []\n",
    "    unique_user_ids = set()\n",
    "\n",
    "    for user_id, headline, date in zip(userIDs, articleHeadlines, dates):\n",
    "        if user_id in multiple_dates_users and user_id not in unique_user_ids and len(unique_user_ids) < limit:\n",
    "            unique_user_ids.add(user_id)\n",
    "        if user_id in unique_user_ids and user_id in multiple_dates_users:\n",
    "            filtered_data.append((user_id, headline, date))\n",
    "\n",
    "    return filtered_data\n",
    "    \n",
    "\n",
    "def append_to_tsv(filtered_data, filename):\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        tsv_writer = csv.writer(file, delimiter='\\t')\n",
    "        for user_id, headline, time in filtered_data:\n",
    "            tsv_writer.writerow([user_id, headline, time])\n",
    "\n",
    "combined_filename = 'combined_data_80k.tsv'\n",
    "\n",
    "filtered_data_train = filter_data(userIDsTrain, articleHeadlinesTrain, datesTrain, 40000)\n",
    "append_to_tsv(filtered_data_train, combined_filename)\n",
    "\n",
    "filtered_data_validation = filter_data(userIDsValidation, articleHeadlinesValidation, datesValidation, 20000)\n",
    "append_to_tsv(filtered_data_validation, combined_filename)\n",
    "\n",
    "filtered_data_test = filter_data(userIDsTest, articleHeadlinesTest, datesTest, 20000)\n",
    "append_to_tsv(filtered_data_test, combined_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(filtered_data):\n",
    "    combined_article_headlines = {}\n",
    "    combined_consumption_times = {}\n",
    "\n",
    "    # Extract unique user IDs\n",
    "    multiple_occurrence_users = set(user_id for user_id, _, _ in filtered_data)\n",
    "\n",
    "    for user_id, headline, time in filtered_data:\n",
    "        if user_id not in combined_article_headlines:\n",
    "            combined_article_headlines[user_id] = []\n",
    "            combined_consumption_times[user_id] = []\n",
    "\n",
    "        if time not in combined_consumption_times[user_id]:\n",
    "            combined_consumption_times[user_id].append(time)\n",
    "            combined_article_headlines[user_id].append([headline])\n",
    "        else:\n",
    "            index = combined_consumption_times[user_id].index(time)\n",
    "            combined_article_headlines[user_id][index].append(headline)\n",
    "\n",
    "    return multiple_occurrence_users, combined_article_headlines, combined_consumption_times\n",
    "\n",
    "multiple_occurrence_users_train, combined_article_headlines_train, combined_consumption_times_train = process_data(filtered_data_train)\n",
    "multiple_occurrence_users_validation, combined_article_headlines_validation, combined_consumption_times_validation = process_data(filtered_data_validation)\n",
    "multiple_occurrence_users_test, combined_article_headlines_test, combined_consumption_times_test = process_data(filtered_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(len(multiple_occurrence_users_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import pad\n",
    "\n",
    "def pad_and_stack(tensor_list):\n",
    "    max_rows = max(tensor.size(0) for tensor in tensor_list)\n",
    "    max_cols = max(tensor.size(1) for tensor in tensor_list)\n",
    "\n",
    "    padded_tensors = []\n",
    "    for tensor in tensor_list:\n",
    "        row_padding = max_rows - tensor.size(0)\n",
    "        col_padding = max_cols - tensor.size(1)\n",
    "\n",
    "        padded_tensor = pad(tensor, (0, col_padding, 0, row_padding))\n",
    "\n",
    "        padded_tensors.append(padded_tensor)\n",
    "\n",
    "    stacked_tensor = torch.stack(padded_tensors)\n",
    "\n",
    "    return stacked_tensor\n",
    "\n",
    "def pad_to_max(*tensors):\n",
    "    if (tensors[0].dim() == 2) : \n",
    "        max_rows = max(tensor.size(0) for tensor in tensors)\n",
    "        max_cols = max(tensor.size(1) for tensor in tensors)\n",
    "\n",
    "        # Pad each tensor and store them in a new list\n",
    "        padded_tensors = []\n",
    "        for tensor in tensors:\n",
    "            row_padding = max_rows - tensor.size(0)\n",
    "            col_padding = max_cols - tensor.size(1)\n",
    "\n",
    "            # Pad the tensor (pad format: [pad_left, pad_right, pad_top, pad_bottom])\n",
    "            padded_tensor = F.pad(tensor, (0, col_padding, 0, row_padding))\n",
    "\n",
    "            padded_tensors.append(padded_tensor)\n",
    "\n",
    "        return tuple(padded_tensors)\n",
    "    \n",
    "    \n",
    "    max_rows = max(tensor.size(0) for tensor in tensors)\n",
    "    max_cols = max(tensor.size(1) for tensor in tensors)\n",
    "    max_depth = max(tensor.size(2) for tensor in tensors)  # Add this line\n",
    "\n",
    "    # Pad each tensor and store them in a new list\n",
    "    padded_tensors = []\n",
    "    for tensor in tensors:\n",
    "        row_padding = max_rows - tensor.size(0)\n",
    "        col_padding = max_cols - tensor.size(1)\n",
    "        depth_padding = max_depth - tensor.size(2)  # Add this line\n",
    "\n",
    "        padded_tensor = F.pad(tensor, (0, depth_padding, 0, col_padding, 0, row_padding))\n",
    "\n",
    "        padded_tensors.append(padded_tensor)\n",
    "\n",
    "    return tuple(padded_tensors)\n",
    "    \n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def days_between_dates(date_str, end_date_str):\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    start_date = datetime.strptime(date_str, date_format)\n",
    "    end_date = datetime.strptime(end_date_str, date_format)\n",
    "    return (end_date - start_date).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "print(len(multiple_occurrence_users_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, lstm_hidden_dim, mlp_hidden_dim, lambda_val, alpha, beta):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        \n",
    "        self.sentence_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        self.lstm = nn.LSTM(lstm_hidden_dim*2, lstm_hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.lambda_val = lambda_val\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, user_ids, article_headlines, consumption_times, curr_time):\n",
    "                \n",
    "        all_embeddings = [torch.tensor(self.sentence_model.encode(headlines)) for headlines in article_headlines]\n",
    "        max_articles = max([emb.shape[0] for emb in all_embeddings])\n",
    "    \n",
    "        # Pad the embeddings to have the same size\n",
    "        padded_embeddings = []\n",
    "        for emb in all_embeddings:\n",
    "            pad_size = max_articles - emb.shape[0]\n",
    "            if pad_size > 0:\n",
    "                pad = torch.zeros(pad_size, emb.shape[1])\n",
    "                padded_emb = torch.cat([emb, pad], dim=0)\n",
    "            else:\n",
    "                padded_emb = emb\n",
    "            padded_embeddings.append(padded_emb)\n",
    "\n",
    "        \n",
    "        all_embeddings = torch.stack(padded_embeddings)\n",
    "        \n",
    "        date_format = \"%Y-%m-%d\"\n",
    "        weighted_sum = torch.zeros_like(all_embeddings[0])\n",
    "        for (times, embedding) in zip(consumption_times, all_embeddings) : \n",
    "            current_date_range = days_between_dates(times, curr_time)\n",
    "            weight = torch.exp(-self.lambda_val * torch.tensor(current_date_range))\n",
    "            weighted_sum += (weight * embedding)\n",
    "                \n",
    "        output = self.mlp(weighted_sum)\n",
    "        p_consistent = output[:, :output.shape[1]// 2]\n",
    "        p_transients = output[:, output.shape[1]//2:]\n",
    "        #lstm_out_consistent, _ = self.lstm(p_consistents)\n",
    "        #lstm_out_transient, _ = self.lstm(p_transients)\n",
    "\n",
    "        #updated_p_consistent = lstm_out_consistent[-1]\n",
    "        #updated_p_transient = lstm_out_transient[-1]\n",
    "\n",
    "        return p_consistent, p_transients\n",
    "    \n",
    "    def use_lstm(self, con, tran) : \n",
    "        combined = torch.cat((con, tran), dim=1).unsqueeze(0)\n",
    "        lstm_output, _ = self.lstm(combined)\n",
    "        lstm_output = lstm_output.squeeze(0)\n",
    "\n",
    "        new_p_consistent = lstm_output[:, :lstm_output.shape[1]// 2]\n",
    "        new_p_transient = lstm_output[:, lstm_output.shape[1]//2:]\n",
    "        \n",
    "        return new_p_consistent\n",
    "\n",
    "    def generate_positive_embeddings(self, user_ids, headlines, consumption_times, curr_time):\n",
    "        random_idx = random.randint(0, len(headlines) - 1)\n",
    "        selected_headlines = headlines[random_idx]\n",
    "        selected_times = consumption_times[random_idx]\n",
    "\n",
    "        subsampled_headlines = random.sample(selected_headlines, len(selected_headlines) // 2)\n",
    "        if len(subsampled_headlines) > 1:\n",
    "            p_u_con, p_u_tra = self.forward(user_ids, [subsampled_headlines], [selected_times], curr_time)\n",
    "        else:\n",
    "            p_u_con, p_u_tra = self.forward(user_ids, [selected_headlines], [selected_times], curr_time)\n",
    "\n",
    "        return torch.cat((p_u_con, p_u_tra), dim=1)\n",
    "\n",
    "    def generate_negative_embeddings(self, target_embedding, all_embeddings):\n",
    "\n",
    "        max_distance = -1\n",
    "        negative_embedding_idx = -1\n",
    "\n",
    "        for idx, emb in enumerate(all_embeddings):\n",
    "            if torch.equal(target_embedding, emb):\n",
    "                continue\n",
    "\n",
    "            distance = torch.norm(target_embedding - emb, p=2)\n",
    "\n",
    "            if distance > max_distance:\n",
    "                max_distance = distance\n",
    "                negative_embedding_idx = idx\n",
    "\n",
    "        return all_embeddings[negative_embedding_idx] if negative_embedding_idx >= 0 else all_embeddings[0]\n",
    "\n",
    "\n",
    "    def compute_loss(self, p_u, p_positive, p_negative, p_u_con, p_u_tra):\n",
    "        listOfTensors = pad_to_max(p_u, p_positive, p_negative)\n",
    "        p_u = listOfTensors[0]\n",
    "        p_positive = listOfTensors[1]\n",
    "        p_negative = listOfTensors[2] \n",
    "        contrastive_loss = torch.relu(torch.norm(p_u - p_positive, p=2)**2 - \n",
    "                              torch.norm(p_u - p_negative, p=2)**2 + \n",
    "                              self.alpha)\n",
    "        \n",
    "        if torch.all(contrastive_loss <= 0):\n",
    "            return 0\n",
    "        \n",
    "        lstmVal = self.use_lstm(p_u_con, p_u_tra)\n",
    "        listOfTensors2 = pad_to_max(lstmVal, p_u_con)\n",
    "        \n",
    "        lstmVal = listOfTensors2[0]\n",
    "        p_u_con = listOfTensors2[1]\n",
    "        \n",
    "        consistency_loss = self.beta * torch.norm(lstmVal - p_u_con, p=2, dim=1)**2\n",
    "        \n",
    "        \n",
    "        return torch.mean(contrastive_loss + consistency_loss)\n",
    "\n",
    "\n",
    "    def training_step(self, users, combined_article_headlines, combined_consumption_times, ttv):\n",
    "        if (ttv == \"train\") : \n",
    "            startDate = \"2017-01-01\"\n",
    "            date = \"2017-12-31\"\n",
    "        if (ttv == \"test\") :\n",
    "            startDate = \"2018-01-01\"\n",
    "            date = \"2018-06-30\"\n",
    "        if (ttv == \"validation\") : \n",
    "            startDate = \"2018-07-01\"\n",
    "            date = \"2018-12-31\"\n",
    "            \n",
    "        \n",
    "        \n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "        currDate = startDate\n",
    "        currDate = datetime.strptime(currDate, \"%Y-%m-%d\")\n",
    "        \n",
    "        \n",
    "        loss = 0 \n",
    "        \n",
    "        while currDate < date : \n",
    "            batch_p = []\n",
    "            batch_p_cons = []\n",
    "            batch_p_transient = []\n",
    "            batch_p_pos = []\n",
    "            batch_p_negatives = []\n",
    "            currDate = currDate.strftime(\"%Y-%m-%d\")\n",
    "            for user_id in users:\n",
    "                \n",
    "                headlines_sequence = combined_article_headlines[user_id]\n",
    "                times_sequence = combined_consumption_times[user_id]                \n",
    "\n",
    "                p_consistent, p_transient = self.forward(user_id, headlines_sequence, times_sequence, currDate)\n",
    "                p_pos = self.generate_positive_embeddings(user_id, headlines_sequence, times_sequence, currDate)\n",
    "                p = torch.cat((p_consistent, p_transient), dim=1)\n",
    "                \n",
    "                \n",
    "                batch_p_cons.append(p_consistent)\n",
    "                batch_p_transient.append(p_transient)\n",
    "                batch_p.append(p)\n",
    "                batch_p_pos.append(p_pos)\n",
    "\n",
    "            batch_p_cons = pad_and_stack(batch_p_cons)\n",
    "            batch_p_pos = pad_and_stack(batch_p_pos)\n",
    "            batch_p = pad_and_stack(batch_p)\n",
    "            batch_p_transient = pad_and_stack(batch_p_transient)\n",
    "\n",
    "            for value in batch_p:\n",
    "                p_negative = self.generate_negative_embeddings(value, batch_p)\n",
    "                batch_p_negatives.append(p_negative)\n",
    "            \n",
    "            #compute_loss(self, p_u, p_positive, p_negative, p_u_con, p_u_tra):\n",
    "            \n",
    "            for input1, input2, input3, input4, input5 in zip (batch_p, batch_p_pos, batch_p_negatives, batch_p_cons, batch_p_transient) : \n",
    "                currLoss = self.compute_loss(input1, input2, input3, input4, input5)\n",
    "                loss += currLoss\n",
    "            \n",
    "            currDate = datetime.strptime(currDate, \"%Y-%m-%d\")\n",
    "            currDate = currDate + timedelta(days=1)\n",
    "            \n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "def train(model, user_ids, article_headlines, consumption_times, optimizer, num_epochs=8, batch_size=128):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    model.train()\n",
    "    \n",
    "    batch_user_ids = user_ids[:256]\n",
    "    \n",
    "    batch_consumption_times = consumption_times\n",
    "    batch_article_headlines = article_headlines\n",
    "    \n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for i in range(0, len(batch_user_ids), batch_size):\n",
    "            mini_batch_user_ids = batch_user_ids[i:i+batch_size]\n",
    "            mini_batch_consumption_times = batch_consumption_times\n",
    "            mini_batch_article_headlines = batch_article_headlines\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model.training_step(mini_batch_user_ids, mini_batch_article_headlines, mini_batch_consumption_times, \"train\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        scheduler.step()\n",
    "        # Calculate and print the average loss per batch\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}, Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "# Assuming the model and data are defined\n",
    "embedding_dim = 768\n",
    "lstm_hidden_dim = 384\n",
    "mlp_hidden_dim = 384\n",
    "lambda_val = 0.5\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "model = UserEncoder(embedding_dim, lstm_hidden_dim, mlp_hidden_dim, lambda_val, alpha, beta)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_users = list(multiple_occurrence_users_train)\n",
    "train(model,mu_users , combined_article_headlines_train, combined_consumption_times_train, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'encoderFinal2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_sequence = []\n",
    "times_sequence = []\n",
    "user_ids_list = ['U102704']\n",
    "user_ids2_list = ['U207812']\n",
    "headlines_sequence_2 = [['Luxury store sets minimum spend for Santa encounters', 'University athlete meets tragic fate in practice session', 'Dairy giant Dean Foods goes bankrupt', 'Lawsuit against firearm maker gets green light from Supreme Court', 'Child succumbs to gunshot wound', 'College freshman dies after incident at fraternity house', 'Evidence of Turkish Forces targeting civilians captured by U.S. Drones', 'Seahawks triumph in nail-biting overtime against 49ers'], ['Parent brings baby to drug transaction, child gets injured', 'Prominent Solar Panel Manufacturer Shuts Down Production', 'For Sale: Deserted Missile Complex in Arizona listed at $400k', 'Firefighter loses life in line of duty in Massachusetts', 'Valiant Officer Pulls Driver From Flaming Wreckage', 'McLaren Reveals the Elva: A Windshield-less Hypercar Priced at $1.7 Million', 'Universities in Hong Kong under siege as police crackdown intensifies']]\n",
    "\n",
    "\n",
    "headlines_sequence = combined_article_headlines['U102704']\n",
    "print (headlines_sequence)\n",
    "print (headlines_sequence_2)\n",
    "times_sequence = combined_consumption_times[user_ids_list[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Disable gradient computation during inference for efficiency\n",
    "    predictions = model(user_ids_list, headlines_sequence, times_sequence)\n",
    "    predictions2 = model(user_ids2_list, headlines_sequence_2, times_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (multiple_occurrence_users[3501])\n",
    "print (predictions[0][7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = predictions2 - predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_tuple = tuple(tensor2 - tensor1 for tensor1, tensor2 in zip(predictions2, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(difference_tuple[1][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_consumption_times = [combined_consumption_times[user_id] for user_id in predict_user_ids]\n",
    "predict_article_headlines = [combined_article_headlines[user_id] for user_id in predict_user_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_consumption_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_identical = all(torch.equal(tensor1, tensor2) for tensor1, tensor2 in zip(predictions, predictions2))\n",
    "\n",
    "print(\"Tuples are identical:\", are_identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_difference = sum(torch.abs(tensor2 - tensor1).sum() for tensor1, tensor2 in zip(predictions2, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_list = ['U102704']\n",
    "user_ids2_list = ['U207812']\n",
    "headlines_sequence_2 = [['Luxury store sets minimum spend for Santa encounters', 'University athlete meets tragic fate in practice session', 'Dairy giant Dean Foods goes bankrupt', 'Lawsuit against firearm maker gets green light from Supreme Court', 'Child succumbs to gunshot wound', 'College freshman dies after incident at fraternity house', 'Evidence of Turkish Forces targeting civilians captured by U.S. Drones', 'Seahawks triumph in nail-biting overtime against 49ers'], ['Parent brings baby to drug transaction, child gets injured', 'Prominent Solar Panel Manufacturer Shuts Down Production', 'For Sale: Deserted Missile Complex in Arizona listed at $400k', 'Firefighter loses life in line of duty in Massachusetts', 'Valiant Officer Pulls Driver From Flaming Wreckage', 'McLaren Reveals the Elva: A Windshield-less Hypercar Priced at $1.7 Million', 'Universities in Hong Kong under siege as police crackdown intensifies']]\n",
    "\n",
    "\n",
    "headlines_sequence = combined_article_headlines['U102704']\n",
    "#print (headlines_sequence)\n",
    "#print (headlines_sequence_2)\n",
    "times_sequence = combined_consumption_times[user_ids_list[0]]\n",
    "\n",
    "with torch.no_grad():  \n",
    "    predictions = model(user_ids_list, headlines_sequence, times_sequence)\n",
    "    predictions2 = model(user_ids2_list, headlines_sequence, times_sequence)\n",
    "\n",
    "    are_identical = all(torch.equal(tensor1, tensor2) for tensor1, tensor2 in zip(predictions, predictions2))\n",
    "\n",
    "print(\"Tuples are identical:\", are_identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_sequence_3 = [['Local bakery introduces new line of vegan pastries', 'Researchers discover new species of deep-sea creatures', 'City plans to open new public library next month', 'Scientists announce breakthrough in renewable energy technology', 'Young pianist wins international music competition', 'University unveils plans for environmentally-friendly campus renovations', 'Rare bird species spotted in local wildlife reserve', '5 highlights from the latest tech expo'], ['Gardener discovers ancient artifact in backyard', 'New planet discovered in our solar system', 'Local artist transforms abandoned building into public art space', 'Veterinarian volunteers to help injured wildlife in rainforest', 'Firefighter adopts dog he rescued from burning building', 'The latest electric car model breaks records for speed and efficiency', 'International Food Festival attracts visitors from around the globe']]\n",
    "\n",
    "with torch.no_grad():  \n",
    "    predictions = model(user_ids_list, headlines_sequence, times_sequence)\n",
    "    predictions2 = model(user_ids2_list, headlines_sequence_3, times_sequence)\n",
    "\n",
    "    are_identical = all(torch.equal(tensor1, tensor2) for tensor1, tensor2 in zip(predictions, predictions2))\n",
    "\n",
    "print(\"Tuples are identical:\", are_identical)\n",
    "\n",
    "total_difference = sum(torch.abs(tensor2 - tensor1).sum() for tensor1, tensor2 in zip(predictions2, predictions))\n",
    "\n",
    "print(total_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_sequence_2 = [10,11]\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(user_ids_list, headlines_sequence, times_sequence)\n",
    "    predictions2 = model(user_ids2_list, headlines_sequence, times_sequence)\n",
    "\n",
    "    are_identical = all(torch.equal(tensor1, tensor2) for tensor1, tensor2 in zip(predictions, predictions2))\n",
    "    \n",
    "total_difference = sum(torch.abs(tensor2 - tensor1).sum() for tensor1, tensor2 in zip(predictions2, predictions))\n",
    "\n",
    "print(total_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperenv",
   "language": "python",
   "name": "whisperenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
