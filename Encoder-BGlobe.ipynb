{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_tsv_to_data(file_name):\n",
    "    userIDs = []\n",
    "    articleHeadlines = []\n",
    "    dates = []\n",
    "\n",
    "    with open(file_name, mode='r', encoding='utf-8') as file:\n",
    "        tsv_reader = csv.reader(file, delimiter='\\t')\n",
    "        for row in tsv_reader:\n",
    "            userID, headline, date = row[0], row[1], row[2]\n",
    "            userIDs.append(userID)\n",
    "            articleHeadlines.append(headline)\n",
    "            dates.append(date)\n",
    "\n",
    "    return userIDs, articleHeadlines, dates\n",
    "\n",
    "def split_data_by_year(userIDs, articleHeadlines, dates):\n",
    "    train = []\n",
    "    validation = []\n",
    "    test = []\n",
    "\n",
    "    train_users = set()\n",
    "    validation_users = set()\n",
    "    test_users = set()\n",
    "\n",
    "    for userID, headline, date in zip(userIDs, articleHeadlines, dates):\n",
    "        if date[:4] == \"2017\":\n",
    "            if len(train_users) < 40000:\n",
    "                train.append((userID, headline, date))\n",
    "                train_users.add(userID)\n",
    "        elif date[:4] == \"2018\" and int(date[5:7]) < 7:\n",
    "            if len(validation_users) < 20000:\n",
    "                validation.append((userID, headline, date))\n",
    "                validation_users.add(userID)\n",
    "        elif date[:4] == \"2018\" and int(date[5:7]) >= 7:\n",
    "            if len(test_users) < 20000:\n",
    "                test.append((userID, headline, date))\n",
    "                test_users.add(userID)\n",
    "\n",
    "    return train, validation, test\n",
    "\n",
    "combined_filename = 'combined_data_80k.tsv'\n",
    "userIDs, articleHeadlines, dates = read_tsv_to_data(combined_filename)\n",
    "\n",
    "filtered_data_train, filtered_data_validation, filtered_data_test = split_data_by_year(userIDs, articleHeadlines, dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def days_between_dates(date_str, end_date_str):\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    start_date = datetime.strptime(date_str, date_format)\n",
    "    end_date = datetime.strptime(end_date_str, date_format)\n",
    "    return (end_date - start_date).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users in training data: 40000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_data(filtered_data, sentence_model):\n",
    "    user_ids = [user_id for user_id, _, _ in filtered_data]\n",
    "    \n",
    "    headlines_to_encode = [headline for _, headline, _ in filtered_data]\n",
    "    encoded_headlines = sentence_model.encode(headlines_to_encode, batch_size=1500)\n",
    "    encoded_headline_iter = iter(encoded_headlines)  \n",
    "\n",
    "    combined_article_headlines = {}\n",
    "    combined_consumption_times = {}\n",
    "\n",
    "    for user_id, _, time in filtered_data:\n",
    "        encoded_headline = next(encoded_headline_iter)  \n",
    "\n",
    "        if user_id not in combined_article_headlines:\n",
    "            combined_article_headlines[user_id] = []\n",
    "            combined_consumption_times[user_id] = []\n",
    "\n",
    "        if time not in combined_consumption_times[user_id]:\n",
    "            combined_consumption_times[user_id].append(time)\n",
    "            combined_article_headlines[user_id].append([encoded_headline])\n",
    "        else:\n",
    "            index = combined_consumption_times[user_id].index(time)\n",
    "            combined_article_headlines[user_id][index].append(encoded_headline)\n",
    "\n",
    "    return user_ids, combined_article_headlines, combined_consumption_times\n",
    "\n",
    "sentence_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Process the data\n",
    "user_ids_train, combined_article_headlines_train, combined_consumption_times_train = process_data(filtered_data_train, sentence_model)\n",
    "user_ids_validation, combined_article_headlines_validation, combined_consumption_times_validation = process_data(filtered_data_validation, sentence_model)\n",
    "user_ids_test, combined_article_headlines_test, combined_consumption_times_test = process_data(filtered_data_test, sentence_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, lstm_hidden_dim, mlp_hidden_dim, lambda_val, alpha, beta):\n",
    "        super(UserEncoder, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(lstm_hidden_dim*2, lstm_hidden_dim*2, batch_first=True)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        self.lambda_val = lambda_val\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, user_ids, all_embeddings, consumption_times, curr_time):\n",
    "                \n",
    "\n",
    "        date_format = \"%Y-%m-%d\"\n",
    "        weighted_sum = torch.zeros((1, 768))\n",
    "        for (times, embedding) in zip(consumption_times, all_embeddings) : \n",
    "            embedding = [torch.from_numpy(np_array) for np_array in embedding]\n",
    "            total_embed = torch.sum(torch.stack(embedding), dim=0)\n",
    "            current_date_range = days_between_dates(times, curr_time)\n",
    "            weight = torch.exp(-self.lambda_val * torch.tensor(current_date_range))\n",
    "            weighted_sum += (weight * total_embed)\n",
    "                \n",
    "        output = self.mlp(weighted_sum)\n",
    "        p_consistent = output[:, :output.shape[1]// 2]\n",
    "        p_transients = output[:, output.shape[1]//2:]\n",
    "        #lstm_out_consistent, _ = self.lstm(p_consistents)\n",
    "        #lstm_out_transient, _ = self.lstm(p_transients)\n",
    "\n",
    "        #updated_p_consistent = lstm_out_consistent[-1]\n",
    "        #updated_p_transient = lstm_out_transient[-1]\n",
    "\n",
    "        return p_consistent, p_transients\n",
    "    \n",
    "    def use_lstm(self, con, tran) : \n",
    "        combined = torch.cat((con, tran), dim=1).unsqueeze(0)\n",
    "        lstm_output, _ = self.lstm(combined)\n",
    "        lstm_output = lstm_output.squeeze(0)\n",
    "\n",
    "        new_p_consistent = lstm_output[:, :lstm_output.shape[1]// 2]\n",
    "        new_p_transient = lstm_output[:, lstm_output.shape[1]//2:]\n",
    "        \n",
    "        return new_p_consistent\n",
    "\n",
    "    def generate_positive_embeddings(self, user_ids, headlines, consumption_times, curr_time):\n",
    "        random_idx = random.randint(0, len(headlines) - 1)\n",
    "        selected_headlines = headlines[random_idx]\n",
    "        selected_times = consumption_times[random_idx]\n",
    "\n",
    "        subsampled_headlines = random.sample(selected_headlines, len(selected_headlines) // 2)\n",
    "        if len(subsampled_headlines) > 1:\n",
    "            p_u_con, p_u_tra = self.forward(user_ids, [subsampled_headlines], [selected_times], curr_time)\n",
    "        else:\n",
    "            p_u_con, p_u_tra = self.forward(user_ids, [selected_headlines], [selected_times], curr_time)\n",
    "\n",
    "        return torch.cat((p_u_con, p_u_tra), dim=1)\n",
    "\n",
    "    def generate_negative_embeddings(self, target_embedding, all_embeddings):\n",
    "\n",
    "        max_distance = -1\n",
    "        negative_embedding_idx = -1\n",
    "\n",
    "        for idx, emb in enumerate(all_embeddings):\n",
    "            if torch.equal(target_embedding, emb):\n",
    "                continue\n",
    "\n",
    "            distance = torch.norm(target_embedding - emb, p=2)\n",
    "\n",
    "            if distance > max_distance:\n",
    "                max_distance = distance\n",
    "                negative_embedding_idx = idx\n",
    "\n",
    "        return all_embeddings[negative_embedding_idx] if negative_embedding_idx >= 0 else all_embeddings[0]\n",
    "\n",
    "\n",
    "    def compute_loss(self, p_u, p_positive, p_negative, p_u_con, p_u_tra):\n",
    "\n",
    "        contrastive_loss = torch.relu(torch.norm(p_u - p_positive, p=2)**2 - \n",
    "                              torch.norm(p_u - p_negative, p=2)**2 + \n",
    "                              self.alpha)\n",
    "        \n",
    "        if torch.all(contrastive_loss <= 0):\n",
    "            return 0\n",
    "        \n",
    "        lstmVal = self.use_lstm(p_u_con, p_u_tra)        \n",
    "        consistency_loss = self.beta * torch.norm(lstmVal - p_u_con, p=2, dim=1)**2\n",
    "        \n",
    "        \n",
    "        return torch.mean(contrastive_loss + consistency_loss)\n",
    "\n",
    "\n",
    "    def training_step(self, users, combined_article_headlines, combined_consumption_times, ttv):\n",
    "        if (ttv == \"train\") : \n",
    "            startDate = \"2017-01-01\"\n",
    "            date = \"2017-12-31\"\n",
    "        if (ttv == \"test\") :\n",
    "            startDate = \"2018-01-01\"\n",
    "            date = \"2018-06-30\"\n",
    "        if (ttv == \"validation\") : \n",
    "            startDate = \"2018-07-01\"\n",
    "            date = \"2018-12-31\"\n",
    "            \n",
    "        \n",
    "        \n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "        currDate = startDate\n",
    "        currDate = datetime.strptime(currDate, \"%Y-%m-%d\")\n",
    "        \n",
    "        \n",
    "        loss = 0 \n",
    "        \n",
    "        while currDate <= date : \n",
    "            batch_p = []\n",
    "            batch_p_cons = []\n",
    "            batch_p_transient = []\n",
    "            batch_p_pos = []\n",
    "            batch_p_negatives = []\n",
    "            currDate = currDate.strftime(\"%Y-%m-%d\")\n",
    "            for user_id in users:\n",
    "                \n",
    "                headlines_sequence = combined_article_headlines[user_id]\n",
    "                times_sequence = combined_consumption_times[user_id]                \n",
    "\n",
    "                p_consistent, p_transient = self.forward(user_id, headlines_sequence, times_sequence, currDate)\n",
    "                p_pos = self.generate_positive_embeddings(user_id, headlines_sequence, times_sequence, currDate)\n",
    "                p = torch.cat((p_consistent, p_transient), dim=1)\n",
    "                \n",
    "                \n",
    "                batch_p_cons.append(p_consistent)\n",
    "                batch_p_transient.append(p_transient)\n",
    "                batch_p.append(p)\n",
    "                batch_p_pos.append(p_pos)\n",
    "\n",
    "\n",
    "            for value in batch_p:\n",
    "                p_negative = self.generate_negative_embeddings(value, batch_p)\n",
    "                batch_p_negatives.append(p_negative)\n",
    "            \n",
    "            #compute_loss(self, p_u, p_positive, p_negative, p_u_con, p_u_tra):\n",
    "            \n",
    "            for input1, input2, input3, input4, input5 in zip (batch_p, batch_p_pos, batch_p_negatives, batch_p_cons, batch_p_transient) : \n",
    "                currLoss = self.compute_loss(input1, input2, input3, input4, input5)\n",
    "                loss += currLoss\n",
    "            \n",
    "            currDate = datetime.strptime(currDate, \"%Y-%m-%d\")\n",
    "            currDate = currDate + timedelta(days=1)\n",
    "            print(\"Done\")\n",
    "            \n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "def train(model, train_user_ids, train_article_headlines, train_consumption_times, \n",
    "          val_user_ids, val_article_headlines, val_consumption_times, \n",
    "          optimizer, num_epochs=8, batch_size=1000):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "    model.train()\n",
    "    \n",
    "    batch_user_ids = train_user_ids[:10000]\n",
    "    \n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for i in range(0, len(train_user_ids), batch_size):\n",
    "            batch_user_ids = train_user_ids[i:i+batch_size]\n",
    "            batch_article_headlines = train_article_headlines\n",
    "            batch_consumption_times = train_consumption_times\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.training_step(batch_user_ids, batch_article_headlines, batch_consumption_times, \"train\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"Batch {num_batches} done\")\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_loss}, Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "    \n",
    "    model.eval()  \n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        val_loss = model.training_step(val_user_ids, val_article_headlines, val_consumption_times, \"validation\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "embedding_dim = 768\n",
    "lstm_hidden_dim = 384\n",
    "mlp_hidden_dim = 384\n",
    "lambda_val = 0.5\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "model = UserEncoder(embedding_dim, lstm_hidden_dim, mlp_hidden_dim, lambda_val, alpha, beta)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_users = list(user_ids_train)\n",
    "train(model, mu_users , combined_article_headlines_train, combined_consumption_times_train, \n",
    "      user_ids_validation, combined_article_headlines_validation, combined_consumption_times_validation, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'encoderFinal2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperenv",
   "language": "python",
   "name": "whisperenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
